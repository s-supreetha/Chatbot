{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens_chain(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        st.write(f'###### Tokens used in this conversation : {cb.total_tokens} tokens')\n",
    "\n",
    "        \n",
    "\n",
    "    return result \n",
    "class Chatbot:\n",
    "    _template = \"\"\"Given the following conversation and a follow-up question, rephrase the follow-up question to be a standalone question.\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow-up entry: {question}\n",
    "    Standalone question:\"\"\"\n",
    "\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "    qa_template = \"\"\"\"You are an AI conversational assistant to answer questions based on a context.\n",
    "    You are given data from a csv file and a question, you must help the user find the information they need. \n",
    "    Your answers should be friendly, response to the user in his own language.\n",
    "    question: {question}\n",
    "    =========\n",
    "    context: {context}\n",
    "    =======\n",
    "    \"\"\"\n",
    "\n",
    "    QA_PROMPT = PromptTemplate(template=qa_template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "    def __init__(self, model_name, temperature, vectors):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.vectors = vectors\n",
    "\n",
    "\n",
    "\n",
    "    def conversational_chat(self, query):\n",
    "            \"\"\"\n",
    "            Starts a conversational chat with a model via Langchain\n",
    "            \"\"\"\n",
    "            llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)\n",
    "            chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=llm,\n",
    "                condense_question_prompt=self.CONDENSE_QUESTION_PROMPT,\n",
    "#                 qa_prompt=self.QA_PROMPT,\n",
    "                retriever=self.vectors.as_retriever(),\n",
    "            )\n",
    "\n",
    "            chain_input = {\"question\": query, \"chat_history\": st.session_state[\"history\"]}\n",
    "            result = chain(chain_input)\n",
    "\n",
    "            st.session_state[\"history\"].append((query, result[\"answer\"]))\n",
    "            count_tokens_chain(chain, chain_input)\n",
    "            return result[\"answer\"]\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
